# elasticsearch-analysis-ik中文分词

## 安装分词器

下载地址：https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v7.8.0

将解压后的后的文件夹放入 ES 根目录下的plugins目录下，重启ES即可使用

![image-20241010105945581](%E5%88%86%E8%AF%8D%E5%99%A8.assets/image-20241010105945581.png)

`ik_smart`：会做最粗粒度的拆分，比如会将"中华人民共和国人民大会堂”拆分为中华人民共和国、人民大会堂"。

`ik_max_word`：比如会将"中华人民共和国人民大会堂" 拆分为"中华人民共和国、中华人民、中华、华人、人民共和国、人民、共和国、大会堂、大会、会堂"等词语。

```json
# GET http://localhost:9200/_analyze 
{ 
  "text":"测试单词", 
  "analyzer":"ik_max_word" 
} 
```

## 配置Ik自定义词典

```json
# GET http://localhost:9200/_analyze 
{ 
  "text":"弗雷尔卓德", 
  "analyzer":"ik_max_word" 
}
```

仅得到每个字的结果

```json
{ 
    "tokens": [ 
        { 
            "token": "弗", 
            "start_offset": 0, 
            "end_offset": 1, 
            "type": "CN_CHAR", 
            "position": 0 
        }, 
        { 
            "token": "雷", 
            "start_offset": 1, 
            "end_offset": 2, 
            "type": "CN_CHAR", 
            "position": 1 
        }, 
        { 
            "token": "尔", 
            "start_offset": 2, 
            "end_offset": 3, 
            "type": "CN_CHAR", 
            "position": 2 
        }, 
        { 
            "token": "卓", 
            "start_offset": 3, 
            "end_offset": 4, 
            "type": "CN_CHAR", 
            "position": 3 
        }, 
        { 
            "token": "德", 
            "start_offset": 4, 
            "end_offset": 5, 
            "type": "CN_CHAR", 
            "position": 4 
        } 
    ] 
}
```

实际我们需要得到的是一个完整的名字：弗雷尔卓德，此时就可以进行扩展词汇，自行定义。

进入 ES 根目录中的 plugins 文件夹下的 ik 文件夹，进入 config 目录，创建 custom.dic （可自行命名）文件，写入弗雷尔卓德。同时打开 IKAnalyzer.cfg.xml 文件，将新建的 custom.dic 配置其中，重启ES服务器。

编辑配置文件，因每次修改词典都需要重启 ES，为解决这不现实的操作，打开远程扩展字典，让支持热词更新（参考：https://blog.csdn.net/chj_1224365967/article/details/120273417）。

![image-20241010111857915](%E5%88%86%E8%AF%8D%E5%99%A8.assets/image-20241010111857915.png)

![image-20241010111934272](%E5%88%86%E8%AF%8D%E5%99%A8.assets/image-20241010111934272.png)

![image-20241010112029289](%E5%88%86%E8%AF%8D%E5%99%A8.assets/image-20241010112029289.png)

![image-20241010112322172](%E5%88%86%E8%AF%8D%E5%99%A8.assets/image-20241010112322172.png)

# 自定义分析器

一个完整的分析器Analyzer包括如下三个组件：

1. Character Filters: 字符过滤器，对文本信息进行预处理，比如对原始文本删减或替换字符

2. Token Filters: 词单元过滤器，分词后过滤，对分词后的词语列表进行处理（过滤，删除等）

3. Tokenizer：分词器，即将原始文本按照一定规则切分为词语（token）

> **Character Filters**
>
> es内置的Character Filters包括：html_strip: 去除html标签, mapping:[字符串替换](https://so.csdn.net/so/search?q=字符串替换&spm=1001.2101.3001.7020)|映射, pattern_replace: 正则匹配替换

> **Token Filters**
>
> Token Filters对Tokenizer分词后的结果进行再加工，包括字符处理，过滤，删除等操作，es内置的Token Filters包括: lowercase(转小写), stop(删除停止词), synonym(添加同义词)。

```json
# char_filter，字符过滤器，名[&_to_and]作用为：将[&]替换为[ and ]
# filter,词单元过滤器,名[my_stopwords]作用为：停用词[the, a]
# analyzer，分词器，名[my_analyzer]作用为：采用“standard”分词器，去除 html 标签，将[&]替换为[and]，字母转小写，停用词[the, a]

# 创建索引 my_index ，指定自定义分析器
# PUT http://localhost:9200/my_index
{
    "settings": {
        "analysis": {
            "char_filter": {
                "&_to_and": {
                    "type": "mapping",
                    "mappings": [
                        "&=> and "
                    ]
                }
            },
            "filter": {
                "my_stopwords": {
                    "type": "stop",
                    "stopwords": [
                        "the",
                        "a"
                    ]
                }
            },
            "analyzer": {
                "my_analyzer": {
                    "type": "custom",
                    "char_filter": [
                        "html_strip",
                        "&_to_and"
                    ],
                    "tokenizer": "standard",
                    "filter": [
                        "lowercase",
                        "my_stopwords"
                    ]
                }
            }
        }
    }
}
```

索引被创建以后，使用  analyze API  验证这个自定义的分析器

```json
# GET http://127.0.0.1:9200/my_index/_analyze 
{ 
    "text":"The <b>quick</b> & Brown fox", 
    "analyzer": "my_analyzer" 
}
```

```json
# 返回报文
{
	"tokens": [
		{
			"token": "quick",
			"start_offset": 7,
			"end_offset": 16,
			"type": "<ALPHANUM>",
			"position": 1
		},
		{
			"token": "and",
			"start_offset": 17,
			"end_offset": 18,
			"type": "<ALPHANUM>",
			"position": 2
		},
		{
			"token": "brown",
			"start_offset": 19,
			"end_offset": 24,
			"type": "<ALPHANUM>",
			"position": 3
		},
		{
			"token": "fox",
			"start_offset": 25,
			"end_offset": 28,
			"type": "<ALPHANUM>",
			"position": 4
		}
	]
}
```

